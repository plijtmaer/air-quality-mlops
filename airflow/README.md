# Apache Airflow - Docker Setup

Este directorio contiene la configuraciÃ³n de Apache Airflow para el proyecto **air-quality-mlops**.

## ğŸ“‹ Estructura

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ hello_airflow.py         # DAG de prueba (smoke test)
â”‚   â”œâ”€â”€ ingest_air_quality.py    # Ingesta desde Open-Meteo API â†’ JSON
â”‚   â””â”€â”€ transform_air_quality.py # TransformaciÃ³n con PySpark â†’ Parquet
â”œâ”€â”€ logs/                        # Logs generados (no se commitean)
â”œâ”€â”€ plugins/                     # Plugins personalizados
â”œâ”€â”€ docker-compose.yaml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                   # Imagen custom con Java + PySpark
â”œâ”€â”€ .env                         # Variables de entorno
â””â”€â”€ README.md                    # Este archivo
```

## ğŸš€ Inicio RÃ¡pido

### Primera Vez (Setup Completo)

```bash
# 1. Navegar a la carpeta airflow
cd airflow

# 2. Construir imagen custom con Java + PySpark (~5 min)
docker compose build

# 3. Levantar todos los servicios
docker compose up -d

# 4. Verificar que todo estÃ¡ corriendo
docker compose ps

# 5. Acceder a la UI
# URL: http://localhost:8080
# Usuario: airflow
# Password: airflow
```

### DespuÃ©s (ya tenÃ©s la imagen)

```bash
cd airflow
docker compose up -d
```

## ğŸ“Š DAGs Disponibles

### 1. `hello_airflow` (Smoke Test)
- **PropÃ³sito**: Verificar que Airflow funciona correctamente
- **Schedule**: `@daily`
- **Tareas**: `start â†’ say_hello â†’ end`

### 2. `ingest_air_quality` (Ingesta)
- **PropÃ³sito**: Descargar datos de calidad del aire desde Open-Meteo API
- **Schedule**: `@hourly`
- **Output**: `data/raw/{city}/{timestamp}.json`
- **Ciudad**: Buenos Aires (-34.6, -58.4)
- **API**: https://open-meteo.com/en/docs/air-quality-api

### 3. `transform_air_quality` (TransformaciÃ³n)
- **PropÃ³sito**: Transformar JSON crudo â†’ Parquet curado con PySpark
- **Schedule**: Cada 6 horas (`0 */6 * * *`)
- **Input**: `data/raw/{city}/*.json`
- **Output**: `data/curated/{city}_air_quality.parquet`
- **Requiere**: Java + PySpark (incluidos en imagen custom)

---

## âš¡ Ejecutar Pipeline Completo

### OpciÃ³n A: Desde la UI Web

1. Abre http://localhost:8080
2. Login: `airflow` / `airflow`
3. Activa `ingest_air_quality` (toggle ON)
4. Click "Trigger DAG" (â–¶ï¸)
5. Espera a que termine (verde = Ã©xito)
6. Activa y ejecuta `transform_air_quality`

### OpciÃ³n B: Desde CLI

```bash
# Trigger ingesta
docker exec airflow-scheduler airflow dags trigger ingest_air_quality

# Esperar unos segundos, luego trigger transformaciÃ³n
docker exec airflow-scheduler airflow dags trigger transform_air_quality

# Ver logs del worker
docker logs airflow-worker --tail 50
```

### Verificar Resultados

```bash
# Ver JSONs crudos
ls ../data/raw/Buenos_Aires/

# Ver Parquet curado
ls ../data/curated/
# DeberÃ­a mostrar: Buenos_Aires_air_quality.parquet/
```

---

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n de Servicios

```bash
# Levantar en background
docker compose up -d

# Ver estado de servicios
docker compose ps

# Ver logs en tiempo real
docker compose logs -f

# Ver logs de un servicio especÃ­fico
docker compose logs -f airflow-worker

# Reiniciar todos los servicios
docker compose restart

# Detener sin eliminar datos
docker compose down

# Reset completo (elimina BD y volÃºmenes)
docker compose down -v
```

### Ejecutar Comandos en Contenedores

```bash
# Acceder a shell del worker
docker exec -it airflow-worker bash

# Listar DAGs
docker exec airflow-scheduler airflow dags list

# Trigger manual de DAG
docker exec airflow-scheduler airflow dags trigger ingest_air_quality

# Verificar Java estÃ¡ instalado
docker exec airflow-worker java -version

# Verificar PySpark estÃ¡ instalado
docker exec airflow-worker python -c "import pyspark; print(pyspark.__version__)"
```

### Base de Datos (PostgreSQL de Airflow)

```bash
# Conectar a PostgreSQL
docker exec -it airflow-postgres psql -U airflow -d airflow

# Ver DAGs registrados
docker exec airflow-postgres psql -U airflow -d airflow -c "SELECT dag_id, is_paused FROM dag;"
```

---

## ğŸ“¦ Servicios Incluidos

| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| `airflow-webserver` | 8080 | UI web de Airflow |
| `airflow-scheduler` | - | Programa y lanza tareas |
| `airflow-worker` | - | Ejecuta tareas (Celery) |
| `airflow-triggerer` | - | Maneja deferrable operators |
| `airflow-postgres` | - | Base de datos de metadatos |
| `airflow-redis` | - | Message broker para Celery |

---

## ğŸ³ Imagen Custom (Dockerfile)

La imagen `airflow-custom:2.10.1-pyspark` incluye:

- **Base**: `apache/airflow:2.10.1-python3.11`
- **Java**: OpenJDK 17 (requerido por Spark)
- **PySpark**: 3.5.3
- **procps**: Para comandos como `ps` (requerido por Spark)

### Reconstruir la Imagen

```bash
# Si modificaste el Dockerfile:
docker compose build --no-cache

# Luego reinicia:
docker compose down
docker compose up -d
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (`.env`)

```bash
AIRFLOW_UID=50000                    # UID del usuario en contenedor
_AIRFLOW_WWW_USER_USERNAME=airflow   # Usuario admin UI
_AIRFLOW_WWW_USER_PASSWORD=airflow   # Password admin UI
```

### Agregar Paquetes Python

Edita el `Dockerfile` para agregar mÃ¡s paquetes:

```dockerfile
RUN pip install --no-cache-dir \
    pyspark==3.5.3 \
    pandas \
    pycaret \
    mlflow
```

Luego reconstruye:
```bash
docker compose build --no-cache
docker compose up -d
```

---

## ğŸ”§ Troubleshooting

### Error de permisos en logs/

```bash
# En Linux/WSL, obtÃ©n tu UID:
id -u

# Actualiza .env con ese valor:
AIRFLOW_UID=1000
```

### DAG no aparece en la UI

1. Verifica que el archivo estÃ© en `dags/`
2. Revisa errores de sintaxis:
   ```bash
   docker compose logs airflow-scheduler | grep -i error
   ```
3. Importa el DAG manualmente para ver errores:
   ```bash
   docker exec airflow-worker python /opt/airflow/dags/ingest_air_quality.py
   ```

### Error "JAVA_HOME is not set"

AsegÃºrate de estar usando la imagen custom:

```bash
# Verificar imagen en uso
docker ps --format "table {{.Names}}\t{{.Image}}"

# Debe mostrar: airflow-custom:2.10.1-pyspark
# NO: apache/airflow:2.10.1-python3.11

# Si no, reconstruye:
docker compose build
docker compose down
docker compose up -d
```

### Error "No raw data found for city"

Ejecuta primero el DAG de ingesta:
```bash
docker exec airflow-scheduler airflow dags trigger ingest_air_quality
# Espera 30 segundos
docker exec airflow-scheduler airflow dags trigger transform_air_quality
```

### Contenedores lentos al iniciar (health: starting)

Es normal la primera vez porque se instalan dependencias. Espera ~2-3 minutos.

Si persiste, verifica los logs:
```bash
docker compose logs airflow-webserver
```

### Reiniciar desde cero

```bash
docker compose down -v          # Elimina contenedores y volÃºmenes
docker compose build --no-cache # Reconstruye imagen
docker compose up -d            # Levanta todo
```

---

## ğŸ“ Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AIRFLOW DAGs                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ingest_air_quality â”‚               â”‚ transform_air_qualityâ”‚
â”‚  (@hourly)          â”‚               â”‚  (cada 6h)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                     â”‚
           â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Open-Meteo API     â”‚               â”‚  PySpark Pipeline   â”‚
â”‚  (gratuita, sin key)â”‚               â”‚  (local[*])         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                     â”‚
           â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  data/raw/          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  data/curated/       â”‚
â”‚  Buenos_Aires/      â”‚               â”‚  Buenos_Aires_       â”‚
â”‚  {timestamp}.json   â”‚               â”‚  air_quality.parquet â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Variables capturadas (por hora):
â€¢ pm2_5, pm10 (partÃ­culas)
â€¢ carbon_monoxide, nitrogen_dioxide, sulphur_dioxide, ozone
â€¢ us_aqi, european_aqi (Ã­ndices)
â€¢ air_quality_label (clasificaciÃ³n: good/moderate/unhealthy)
```

---

## ğŸ§¹ Limpieza

### Detener servicios (preserva datos)

```bash
docker compose down
```

### Eliminar todo (BD, logs, volÃºmenes)

```bash
docker compose down -v
```

### Eliminar imÃ¡genes (libera ~6GB)

```bash
docker rmi airflow-custom:2.10.1-pyspark apache/airflow:2.10.1-python3.11 postgres:15 redis:7
```

### Limpiar cachÃ© de Docker

```bash
docker system prune -a
```
